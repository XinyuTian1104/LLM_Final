{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_train.head()\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_test.head()\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = df_train.copy()\n",
    "test_data = df_test.copy()\n",
    "\n",
    "# Split the train data into train and validation\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts = train_data['text'].values\n",
    "train_labels = train_data['label'].values\n",
    "val_texts = val_data['text'].values\n",
    "val_labels = val_data['label'].values\n",
    "test_texts = test_data['text'].values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "print('Train data:', train_texts.shape, train_labels.shape)\n",
    "print('Validation data:', val_texts.shape, val_labels.shape)\n",
    "print('Test data:', test_texts.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text and for every doc, we only take the first 100 words\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # delete extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # delete single characters\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    # only take the first 100 words\n",
    "    text = ' '.join(text.split()[:100])\n",
    "    return text\n",
    "\n",
    "train_texts = [clean_text(text) for text in train_texts]\n",
    "val_texts = [clean_text(text) for text in val_texts]\n",
    "test_texts = [clean_text(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "vocabulary = Counter()\n",
    "for doc in train_texts:\n",
    "    vocabulary.update(doc.split())\n",
    "vocab_size = len(vocabulary)\n",
    "print(vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign an index to each word\n",
    "word_to_idx = {word: i+1 for i, word in enumerate(vocabulary)} # Starting index from 1, 0 is reserved for padding\n",
    "print(list(word_to_idx.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [[word_to_idx[word] for word in doc.split()] for doc in train_texts]\n",
    "print(train_texts[0])\n",
    "\n",
    "val_texts = [[word_to_idx.get(word, 0) for word in doc.split()] for doc in val_texts]\n",
    "test_texts = [[word_to_idx.get(word, 0) for word in doc.split()] for doc in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_length = 256\n",
    "train_texts = [doc + [0]*(max_length-len(doc)) for doc in train_texts]\n",
    "val_texts = [doc + [0]*(max_length-len(doc)) for doc in val_texts]\n",
    "test_texts = [doc + [0]*(max_length-len(doc)) for doc in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.tensor(train_texts, dtype=torch.long)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.float32)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.tensor(val_texts, dtype=torch.long)\n",
    "y_val = torch.tensor(val_labels, dtype=torch.float32)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "X_test = torch.tensor(test_texts, dtype=torch.long)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.float32)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out = nn.Linear(embedding_dim, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        return torch.sigmoid(self.out(embedded.mean(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SentimentAnalysisModel(vocab_size+1, 100, 1)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0].shape)\n",
    "# build a data loader\n",
    "train_dataset = list(zip(X_train, y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore', \"Intel MKL WARNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move everything to the GPU\n",
    "device = torch.device('cuda')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_loss_max = 100\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        texts, labels = batch\n",
    "        # Move data to the same device as the model\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Prevent optimizer from updating the first embedding vector\n",
    "        model.embedding.weight.data[0] = 0\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move validation data to the same device as the model\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        output = model(X_val)\n",
    "        loss = criterion(output, y_val.unsqueeze(1))\n",
    "        val_loss = loss.item()\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < val_loss_max:\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            val_loss_max = val_loss\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "            if count > 5:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use the best model to evaluate the test data\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(X_test)\n",
    "    loss = criterion(output, y_test.unsqueeze(1))\n",
    "    print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "    preds = (output > 0.5).long()\n",
    "    accuracy = (preds == y_test.unsqueeze(1)).sum().float() / len(y_test)\n",
    "    print(f'Test Accuracy: {accuracy.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
